{
  "metadata": {
    "name": "P2 - Scala RDD vs CSV - Scala, PySpark \u0026 R - Setting Schema vs TempView",
    "kernelspec": {
      "language": "scala",
      "name": "spark2-scala"
    },
    "language_info": {
      "codemirror_mode": "text/x-scala",
      "file_extension": ".scala",
      "mimetype": "text/x-scala",
      "name": "scala",
      "pygments_lexer": "scala"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 2,
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "Multi-purpose Notebook\n\nThe Notebook is the place for all your needs\n\n    Data Ingestion\n    Data Discovery\n    Data Analytics\n    Data Visualization \u0026 Collaboration\n\n\n"
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nutil.Properties.versionString"
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nsc.version"
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": "    val is constant value and not mutable\n    var is variable and mutable"
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n//Read Input from Text File (5GB) .. can be downloaded from the link given below\n//https://dataverse.harvard.edu/dataset.xhtml?persistentId\u003ddoi:10.7910/DVN/HG7NV7 \n\nvar rdd \u003d sc.textFile(\"file:///downloads/Data/\")"
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval header \u003d rdd.first()"
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nrdd \u003d rdd.filter(row \u003d\u003e row !\u003d header)"
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n//Prints no. of rows\nrdd.count()\n\n//Prints first 10 rows\nrdd.take(10).foreach(println)"
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\n// One method for defining the schema of an RDD is to make a case class with the desired column names and types.\ncase class Flight(Year: String, Month: String, DayofMonth: String, DayOfWeek: String, DepTime: String, CRSDepTime: String, ArrTime: String, CRSArrTime: String, UniqueCarrier: String, FlightNum: String, TailNum: String, ActualElapsedTime: String, CRSElapsedTime: String, AirTime: String, ArrDelay: String, DepDelay: String, Origin: String, Dest: String, Distance: String, TaxiIn: String, TaxiOut: String, Cancelled: String, CancellationCode: String, Diverted: String, CarrierDelay: String, WeatherDelay: String, NASDelay: String, SecurityDelay: String, LateAircraftDelay: String)\n\nvar new_rdd \u003d rdd.map(_.split(\",\")).map(p \u003d\u003e Flight(p(0), p(1), p(2), p(3), p(4), p(5), p(6), p(7), p(8), p(9), p(10), p(11), p(12), p(13), p(14), p(15), p(16), p(17), p(18), p(19), p(20), p(21), p(22), p(23), p(24), p(25), p(26), p(27), p(28)))\n\n// Any RDD containing case classes can be registered as a table.  The schema of the table is automatically inferred using scala reflection.\nvar scala_df \u003d new_rdd.toDF()\n\nscala_df.createOrReplaceTempView(\"scala_flight\") // \u0027scala_flight\u0027 table can be accessed by any language within zeppelin.\n\nscala_df.printSchema()"
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval FlightSQL \u003d sqlContext.sql(\"SELECT * FROM scala_flight LIMIT 5\")"
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nFlightSQL.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nz.put(\"scala_df\", scala_df) // z is zeppelin object"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\nimport sys\nprint(sys.version)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# One method of getting data from Scala is to use z.get()\nfrom pyspark.sql import SQLContext, DataFrame, Row\nsqlContext \u003d SQLContext(sc)\n\npyspark_df \u003d DataFrame(z.get(\"scala_df\"), sqlContext) #PySpark DataFrame\ntype(pyspark_df)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\npyspark_df \u003d spark.sql(\"SELECT * FROM scala_flight LIMIT 10000\")  #retrieving 10,000 rows from \u0027scala_flight\u0027 table which is registered as TempView in Scala\ntype(pyspark_df)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# Prints no. of rows read from Scala\npyspark_df.count()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\n# Prints no. of rows in \u0027scala_flight\u0027 table using SQL\nspark.sql(\"SELECT COUNT(*) FROM scala_flight\").show() # Recall scala_flight is accessible by any language within zeppelin"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\npyspark_df.printSchema()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\npyspark_df.head(5)"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\nFlightSQL \u003d spark.sql(\"SELECT Year, Month, COUNT(*) FROM scala_flight GROUP BY Year, Month ORDER BY Year, Month DESC\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\nFlightSQL.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\n\nval flightDF \u003d spark.read.format(\"csv\").option(\"sep\", \",\").option(\"inferSchema\", \"true\").option(\"header\", \"true\").load(\"file:///downloads/Data/\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\ndf \u003d spark.read.load(\"file:///downloads/Data/\", format\u003d\"csv\", sep\u003d\",\", inferSchema\u003d\"true\", header\u003d\"true\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\ndf.head()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\ndf.count()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\ndf.createOrReplaceTempView(\"pyspark_csv_flight\")\nsqlDF \u003d spark.sql(\"SELECT Year, Month, COUNT(*) FROM pyspark_csv_flight GROUP BY Year, Month\")"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%pyspark\n\nsqlDF.show()"
    },
    {
      "cell_type": "code",
      "execution_count": 27,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nspark.sql(\"\"\"SELECT Distance, Origin, Dest \n        FROM scala_flight WHERE Distance \u003e 1000 \n        ORDER BY Distance DESC\"\"\").show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 28,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nspark.sql(\"\"\"SELECT DayofMonth, ArrDelay, Origin, Dest \n        FROM scala_flight \n        WHERE ArrDelay \u003e 120 AND Origin \u003d \u0027SFO\u0027 AND Dest \u003d \u0027ORD\u0027 \n        ORDER by ArrDelay DESC\"\"\").show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 29,
      "metadata": {
        "autoscroll": "auto"
      },
      "outputs": [],
      "source": "%spark\nspark.sql(\"\"\"SELECT ArrDelay, Origin, Dest,\n         CASE\n         WHEN ArrDelay \u003e 360 THEN \u0027Very Long Delays\u0027\n         WHEN ArrDelay \u003e 120 AND ArrDelay \u003c 360 THEN \u0027Long Delays\u0027\n         WHEN ArrDelay \u003e 60 AND ArrDelay \u003c 120 THEN \u0027Short Delays\u0027\n         WHEN ArrDelay \u003e 0 and ArrDelay \u003c 60 THEN \u0027Tolerable Delays\u0027\n         WHEN ArrDelay \u003d 0 THEN \u0027No Delays\u0027\n         ELSE \u0027Early\u0027\n         END AS Flight_Delays\n         FROM scala_flight\n         ORDER BY Origin, ArrDelay DESC\"\"\").show(10)\n"
    },
    {
      "cell_type": "code",
      "execution_count": 30,
      "metadata": {},
      "outputs": [],
      "source": "%%sql select UniqueCarrier, avg(DepDelay)\n from scala_flight\n group by UniqueCarrier\n"
    },
    {
      "cell_type": "code",
      "execution_count": 31,
      "metadata": {},
      "outputs": [],
      "source": "%%sql\nselect UniqueCarrier, count(DepDelay)\nfrom scala_flight where DepDelay \u003e 40\ngroup by UniqueCarrier\n"
    },
    {
      "cell_type": "code",
      "execution_count": 32,
      "metadata": {},
      "outputs": [],
      "source": "%%sql\r\nselect Origin, Dest, count(DepDelay)\r\nfrom scala_flight where DepDelay \u003e 40\r\ngroup by Origin, Dest\r\nORDER BY count(DepDelay) desc"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%r\n\nR.Version()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%r\n\nsparkR.session()"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%r\n\ndf \u003c- sql(\"SELECT * FROM scala_flight LIMIT 10000\") # Reading 10000 rows"
    },
    {
      "cell_type": "raw",
      "metadata": {
        "format": "text/plain"
      },
      "source": "%r\n\nhead(df)"
    }
  ]
}